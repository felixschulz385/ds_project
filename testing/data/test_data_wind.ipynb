{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base url \n",
    "wind_data_url = \"https://opendata.dwd.de/climate_environment/CDC/grids_germany/hourly/Project_TRY/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(): \n",
    "    \"\"\"Get all links which contain data on wind from base url.\"\"\"\n",
    "    \n",
    "    # create response object \n",
    "    r = requests.get(wind_data_url)\n",
    "    \n",
    "    # create beautiful-soup object \n",
    "    soup = BeautifulSoup(r.content,'html.parser') \n",
    "    \n",
    "    # find all links on web-page \n",
    "    links = soup.findAll('a') \n",
    "    \n",
    "    # find all links from web-page where wind data is contained \n",
    "    wind_links = [wind_data_url + link['href'] for link in links if bool(re.search(\"wind\", link['href']))]  \n",
    "     \n",
    "    return wind_links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(wind_links): \n",
    "    \"\"\"Download files from links with wind data.\"\"\"\n",
    "    \n",
    "    # initialize empty dictionary \n",
    "    download_links = {}\n",
    "\n",
    "    # loop over directories which contain wind data \n",
    "    for wind_link in wind_links: \n",
    "        \n",
    "        directory_name = wind_link.split('/')[-2] \n",
    "        \n",
    "        #create response object\n",
    "        r = requests.get(wind_link)\n",
    "\n",
    "        # create beautiful-soup object \n",
    "        soup = BeautifulSoup(r.content,'html.parser')\n",
    "\n",
    "        # find all links on web-pag\n",
    "        links = soup.findAll('a')\n",
    "        \n",
    "        # dictionary with download links as values and directory name as key \n",
    "        download_links[directory_name] = [wind_link + link['href'] for link in links if link['href'].endswith('.nc.gz')]\n",
    "        \n",
    "    # outer loop: wind links, i.e. wind speed and wind direction  \n",
    "    for key in download_links: \n",
    "        # specify directory to store data to \n",
    "        directory = os.path.join('/pfs/work7/workspace/scratch/tu_zxobe27-ds_project/data', key)\n",
    "        \n",
    "        try:\n",
    "            # try to make directory if it does not exist\n",
    "            os.mkdir(directory)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # inner loop: file links within wind links\n",
    "        for link in download_links[key]:\n",
    "            # obtain filename by splitting url and getting \n",
    "            # last string \n",
    "            file_name = link.split('/')[-1] \n",
    "            \n",
    "            # check whether file exists in path. If false -> download file to path\n",
    "            if not os.path.exists(os.path.join(directory, file_name)): \n",
    "                # create response object \n",
    "                r = requests.get(link, stream = True) \n",
    "                \n",
    "                # start download  \n",
    "                with open(os.path.join(directory, file_name), 'wb') as f: \n",
    "                    for chunk in r.iter_content(chunk_size = 1024*1024): \n",
    "                        if chunk: \n",
    "                            f.write(chunk) \n",
    "                \n",
    "                print( f\"{file_name} downloaded.\", end='\\r')\n",
    "            \n",
    "            # if file exists pass \n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    print (\"All files downloaded\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "  \n",
    "    # getting all video links \n",
    "    wind_links = get_links()\n",
    "    \n",
    "    #cdownload wind data \n",
    "    download_files(wind_links=wind_links)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "488307011aaf990456e74b9f2d65811f2d14a922aa5ea10f6ce2a4be1284880c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('ds_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
