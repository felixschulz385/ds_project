{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os \n",
    "import gzip\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base url \n",
    "wind_data_url = \"https://opendata.dwd.de/climate_environment/CDC/grids_germany/hourly/Project_TRY/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(): \n",
    "    \"\"\"Get all links which contain data on wind from base url.\"\"\"\n",
    "    \n",
    "    # create response object \n",
    "    r = requests.get(wind_data_url)\n",
    "    \n",
    "    # create beautiful-soup object \n",
    "    soup = BeautifulSoup(r.content,'html.parser') \n",
    "    \n",
    "    # find all links on web-page \n",
    "    links = soup.findAll('a')\n",
    "    \n",
    "    # specify data to consider \n",
    "    matches = [\"wind\", \"pressure\", \"cloud\"]\n",
    "    parent_links = []   # initialize empty list \n",
    "\n",
    "    # loop over links and create list with links to download from \n",
    "    for link in links: \n",
    "        if any(x in link['href'] for x in matches):\n",
    "            if \"vapor\" in link['href']:\n",
    "                pass \n",
    "            else:\n",
    "                parent_links.append(wind_data_url + link['href'])\n",
    "     \n",
    "    return parent_links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(parent_links): \n",
    "    \"\"\"Download files from links with wind data.\"\"\"\n",
    "    \n",
    "    # initialize empty dictionary \n",
    "    download_links = {}\n",
    "\n",
    "    # loop over directories which contain wind data \n",
    "    for parent_link in parent_links: \n",
    "        \n",
    "        directory_name = parent_link.split('/')[-2] \n",
    "        \n",
    "        #create response object\n",
    "        r = requests.get(parent_link)\n",
    "\n",
    "        # create beautiful-soup object \n",
    "        soup = BeautifulSoup(r.content,'html.parser')\n",
    "\n",
    "        # find all links on web-pag\n",
    "        links = soup.findAll('a')\n",
    "        \n",
    "        # dictionary with download links as values and directory name as key \n",
    "        download_links[directory_name] = [parent_link + link['href'] for link in links if link['href'].endswith('.nc.gz')]\n",
    "        \n",
    "    # outer loop: wind links, i.e. wind speed and wind direction  \n",
    "    for key in download_links: \n",
    "        # specify directory to store data to \n",
    "        directory = os.path.join('/pfs/work7/workspace/scratch/tu_zxobe27-ds_project/data', key)\n",
    "        \n",
    "        try:\n",
    "            # try to make directory if it does not exist\n",
    "            os.mkdir(directory)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # inner loop: file links within wind links\n",
    "        for link in download_links[key]:\n",
    "            # obtain filename by splitting url and getting \n",
    "            # last string \n",
    "            file_name = link.split('/')[-1] \n",
    "            \n",
    "            # check whether file without .gz exists in path. If false -> download file to path\n",
    "            if not os.path.exists(os.path.splitext(os.path.join(directory, file_name))[0]): \n",
    "                # create response object \n",
    "                r = gzip.open(io.BytesIO(requests.get(link, stream = True).content)).read() \n",
    "                \n",
    "                # download file and unzip .gz file\n",
    "                with open(os.path.splitext(os.path.join(directory, file_name))[0], 'wb') as f: \n",
    "                    f.write(r)\n",
    "                \n",
    "                # # download file\n",
    "                # with open(os.path.join(directory, file_name), 'wb') as f: \n",
    "                #     for chunk in r.iter_content(chunk_size = 1024*1024): \n",
    "                #         if chunk: \n",
    "                #             f.write(chunk)\n",
    "\n",
    "                \n",
    "                print( f\"{file_name} downloaded.\", end='\\r')\n",
    "            \n",
    "            # if file exists pass \n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    print (\"All files downloaded\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloadedloaded.d.\r"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "  \n",
    "    # getting all video links \n",
    "    parent_links = get_links()\n",
    "    \n",
    "    # download data and store files \n",
    "    download_files(parent_links=parent_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
