{"cells":[{"cell_type":"markdown","metadata":{"id":"ktnBrgku7cYT"},"source":["# Surface Image Segmentation "]},{"cell_type":"markdown","metadata":{"id":"EalxNCpG7cYU"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4610,"status":"ok","timestamp":1676027151346,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"LFv1WDAW7cYV"},"outputs":[],"source":["import numpy as np\n","\n","from tqdm import tqdm\n","import glob\n","\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch.utils.data import DataLoader, random_split, Dataset\n","\n","import torchvision\n","import torchvision.transforms.functional as TF\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":224,"status":"ok","timestamp":1676030550807,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"FyOMgnX72aMH"},"outputs":[],"source":["from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n","from torchvision.models.segmentation.fcn import FCNHead\n","from torchvision.models.segmentation import DeepLabV3_ResNet101_Weights\n","from torchvision import models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TfL8pHZvdLO5"},"outputs":[],"source":["from torchvision.models.segmentation.fcn import _fcn_resnet\n","from torchgeo.models import resnet, resnet50"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KOrP_PAv71lw"},"outputs":[],"source":["# Cluster\n","path = \"/pfs/work7/workspace/scratch/tu_zxobe27-ds_project/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3stFDvog7cYW","outputId":"a4af4b6e-5922-449f-8c68-2b81de317942"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"gV_cO9ee7cYW"},"source":["## U-Net Model - Base Model (NOT pre-trained)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":218,"status":"ok","timestamp":1676027193731,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"eU-o0kCN7cYX"},"outputs":[],"source":["# double convolutional layer which is executed in every step of the u-net \n","# conv layer takes as input number of input channels -> in_channels and outputs vice versa\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(DoubleConv, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","    # forward pass in the conv layer \n","    def forward(self, x):\n","        return self.conv(x)\n","\n","# design complete u-net shape \n","# model takes as default 3 input channels and 6 output channels\n","class UNET(nn.Module):\n","    def __init__(\n","            self, in_channels=3, out_channels=6, features=[64, 128, 256, 512],  # features -> num of input nodes at every stage in the model \n","    ):\n","        super(UNET, self).__init__()\n","        self.downs = nn.ModuleList()\n","        self.ups = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # Down part of UNET\n","        for feature in features:\n","            self.downs.append(DoubleConv(in_channels, feature))\n","            in_channels = feature\n","\n","        # Up part of UNET\n","        for feature in reversed(features):  # reverse the features i.o. to move upwards in the model \n","            self.ups.append(\n","                nn.ConvTranspose2d(\n","                    feature*2, feature, kernel_size=2, stride=2,\n","                )\n","            )\n","            self.ups.append(DoubleConv(feature*2, feature))\n","        \n","        # lowest stage in u-net \n","        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n","        # final conv layer: takes in 64 channels and outputs 1 channel by default \n","        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n","\n","    # forward pass of the u-net model between stages \n","    def forward(self, x):\n","        skip_connections = []  # red arrows in the model representation \n","\n","        for down in self.downs:\n","            x = down(x)  # one DoubleConv run-through \n","            skip_connections.append(x)\n","            x = self.pool(x)\n","\n","        x = self.bottleneck(x)\n","        skip_connections = skip_connections[::-1]\n","\n","        for idx in range(0, len(self.ups), 2):\n","            x = self.ups[idx](x)\n","            skip_connection = skip_connections[idx//2]\n","\n","            if x.shape != skip_connection.shape:\n","                x = TF.resize(x, size=skip_connection.shape[2:])\n","\n","            concat_skip = torch.cat((skip_connection, x), dim=1)\n","            x = self.ups[idx+1](concat_skip)\n","\n","        return self.final_conv(x)"]},{"cell_type":"markdown","metadata":{"id":"GJ90XNO87cYY"},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7434,"status":"ok","timestamp":1676027202203,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"sxqQX0t_7cYY"},"outputs":[],"source":["# List all images and masks/labels \n","imgs = glob.glob(path+'data/model_training/2_Ortho_RGB/sliced/*tif')\n","labels = glob.glob(path+'data/model_training/Labels_all/sliced/*tif')\n","\n","# Create dictionary -> {key: 'link/to/image_or_label'}\n","labels_dict = {label.split(\"/\")[-1].split(\".\")[0].rsplit('_', 1)[0] : label for label in labels}\n","imgs_dict = {img.split(\"/\")[-1].split(\".\")[0].rsplit('_', 1)[0] : img for img in imgs}\n","\n","keys = sorted(list(set(imgs_dict)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1676027202203,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"V30H3jMw7cYY","outputId":"8eebd53f-9d7a-4bac-ccb3-27007a1f67da"},"outputs":[],"source":["%%script echo skipping \n","\n","# check if keys in both dicts are the same \n","print(set(imgs_dict) == set(labels_dict))\n","\n","# inspect dicts\n","print(keys[:5])\n","print(labels_dict[keys[0]])\n","print(imgs_dict[keys[0]])"]},{"cell_type":"markdown","metadata":{},"source":["### Sample"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":378},"executionInfo":{"elapsed":2153,"status":"ok","timestamp":1676027326565,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"tiVCckt17cYZ","outputId":"1cb6fa27-a405-4e13-db24-cbedc1ace1ac"},"outputs":[],"source":["im1 = Image.open(labels_dict[keys[0]]).resize((2000, 2000))\n","im2 = Image.open(imgs_dict[keys[0]]).resize((2000, 2000))\n","\n","f, ax = plt.subplots(1, 2, figsize=(10,5), sharey=True)\n","ax[0].imshow(im1) \n","ax[1].imshow(im2) \n","\n","f.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"W3EUHlRY7cYZ"},"source":["- Impervious surfaces (RGB: 255, 255, 255)      | white \n","- Building (RGB: 0, 0, 255)                     | blue \n","- Low vegetation (RGB: 0, 255, 255)             | light-blue\n","- Tree (RGB: 0, 255, 0)                         | green            \n","- Car (RGB: 255, 255, 0)                        | yellow\n","- Clutter/background (RGB: 255, 0, 0)           | red"]},{"cell_type":"markdown","metadata":{"id":"TRxj24J17cYZ"},"source":["### Custom data set class"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1676027203297,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"aXmpmRpn7cYZ"},"outputs":[],"source":["class PotsdamDataset(Dataset):\n","    def __init__(self, imgs_dict, labels_dict, keys, transform=None):\n","        self.img_dir = imgs_dict\n","        self.mask_dir = labels_dict\n","        self.keys = keys\n","        self.transform = transform\n","        \n","        self.RGB_classes = {\n","            'imprevious' : [255, 255, 225],\n","            'building' : [0,  0, 255],\n","            'low_vegetation' : [0, 255, 255],\n","            'tree' : [0,  255,  0], \n","            'car' : [ 255, 255, 0],\n","            'background' : [255, 0, 0]\n","            }  # in RGB\n","        \n","        self.bin_classes = ['imprevious', 'building', 'low_vegetation', 'tree', 'car', 'background']\n","\n","    def __len__(self):\n","        return len(self.keys)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_dir[self.keys[idx]]\n","        mask_path = self.mask_dir[self.keys[idx]]\n","        \n","        image = np.array(Image.open(img_path).convert(\"RGB\"))\n","        mask = np.array(Image.open(mask_path).convert(\"RGB\"))\n","        \n","        cls_mask = np.zeros(mask.shape) # dim: (6000, 6000, 3)\n","        \n","        # Six to four classes: (imprevious + car + background) joined\n","        cls_mask[(mask == self.RGB_classes['imprevious']).all(-1)] = self.bin_classes.index('imprevious')\n","        cls_mask[(mask == self.RGB_classes['building']).all(-1)] = self.bin_classes.index('building')\n","        cls_mask[(mask == self.RGB_classes['low_vegetation']).all(-1)] = self.bin_classes.index('low_vegetation')\n","        cls_mask[(mask == self.RGB_classes['tree']).all(-1)] = self.bin_classes.index('tree')\n","        cls_mask[(mask == self.RGB_classes['car']).all(-1)] = self.bin_classes.index('imprevious')\n","        cls_mask[(mask == self.RGB_classes['background']).all(-1)] = self.bin_classes.index('imprevious')\n","        cls_mask = cls_mask[:,:,0] # omit last dimension (, , 3) -> RGB  \n","\n","        if self.transform is not None:\n","            augmentations = self.transform(image=image, mask=cls_mask)\n","            image = augmentations[\"image\"]\n","            mask = augmentations[\"mask\"]\n","\n","        return image, mask"]},{"cell_type":"markdown","metadata":{"id":"DEAfz1Bi7cYZ"},"source":["#### Testing the class"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1022,"status":"ok","timestamp":1676027204315,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"1UD2nlVF7cYZ","outputId":"bcea3a61-f5e6-4d9c-a62c-77a74be933fb"},"outputs":[],"source":["y = np.array(Image.open(labels_dict[keys[0]]))\n","x = np.zeros(y.shape)\n","\n","RGB_classes = {\n","            'imprevious' : [255, 255, 225],\n","            'building' : [0,  0, 255],\n","            'low_vegetation' : [0, 255, 255],\n","            'tree' : [0,  255,  0], \n","            'car' : [ 255, 255, 0],\n","            'background' : [255, 0, 0]\n","            }  # in RGB\n","\n","bin_classes = ['imprevious', 'building', 'low_vegetation', 'tree', 'car', 'background']\n","\n","x[(y == RGB_classes['imprevious']).all(-1)] = bin_classes.index('imprevious')\n","x[(y == RGB_classes['building']).all(-1)] = bin_classes.index('building')\n","x[(y == RGB_classes['low_vegetation']).all(-1)] = bin_classes.index('low_vegetation')\n","x[(y == RGB_classes['tree']).all(-1)] = bin_classes.index('tree')\n","x[(y == RGB_classes['car']).all(-1)] = bin_classes.index('car')\n","x[(y == RGB_classes['background']).all(-1)] = bin_classes.index('background')\n","x = x[:,:,0]\n","\n","x[:15, :15]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":517},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1676027204315,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"ETFRghC-7cYa","outputId":"70e1e511-a938-4d6c-b2ed-25fdd34a1ee1"},"outputs":[],"source":["im = Image.open(labels_dict[keys[0]])\n"," \n","# Setting the points for cropped image\n","left = 0\n","top = 0\n","right = 15\n","bottom = 15\n","\n","# Cropped image of above dimension\n","# (It will not change original image)\n","im1 = im.crop((left, top, right, bottom))\n"," \n","im1.resize((500, 500))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":517},"executionInfo":{"elapsed":913,"status":"ok","timestamp":1676027205224,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"SRFHNmtf7cYa","outputId":"d35ec23e-9eb8-42d5-b782-19b54c7df472"},"outputs":[],"source":["im = Image.open(imgs_dict[keys[0]])\n"," \n","# Setting the points for cropped image\n","left = 0\n","top = 0\n","right = 1000\n","bottom = 1000\n","\n","# Cropped image of above dimension\n","# (It will not change original image)\n","im1 = im.crop((left, top, right, bottom))\n","im1.resize((500, 500))"]},{"cell_type":"markdown","metadata":{"id":"GrE84IQu7cYa"},"source":["## Utils "]},{"cell_type":"markdown","metadata":{"id":"UAB_U9AF7cYa"},"source":["### Train-/Test-Split"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":213,"status":"ok","timestamp":1676027332752,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"et9Z5fJX7cYa"},"outputs":[],"source":["dataset = PotsdamDataset(imgs_dict, labels_dict, keys)\n","validation_split = .2\n","shuffle_dataset = True\n","random_seed= 42\n","\n","# Creating data indices for training and validation splits:\n","dataset_size = len(dataset)\n","indices = list(range(dataset_size))\n","split = int(np.floor(validation_split * dataset_size))\n","\n","if shuffle_dataset :\n","    np.random.seed(random_seed)\n","    np.random.shuffle(indices)\n","    \n","train_indices, val_indices = indices[split:], indices[:split]\n","\n","# Creating PT data samplers and loaders:\n","train_sampler = SubsetRandomSampler(train_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)"]},{"cell_type":"markdown","metadata":{"id":"VLy5RBoD7cYa"},"source":["### Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1676027332951,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"RksxzTzF7cYa"},"outputs":[],"source":["def get_loaders(\n","    imgs_dict,\n","    labels_dict,\n","    keys,\n","    batch_size,\n","    train_transform,\n","    val_transform,\n","    num_workers = 2,\n","    pin_memory = True,\n","):\n","    \n","    train_data = PotsdamDataset(\n","        imgs_dict = imgs_dict,\n","        labels_dict = labels_dict,\n","        keys = keys, \n","        transform = train_transform,\n","    )\n","    \n","    valid_data = PotsdamDataset(\n","        imgs_dict = imgs_dict,\n","        labels_dict = labels_dict,\n","        keys = keys, \n","        transform = val_transform,\n","    )\n","\n","    train_loader = DataLoader(\n","        train_data,\n","        batch_size = batch_size,\n","        num_workers = num_workers,\n","        pin_memory = pin_memory,\n","        sampler = train_sampler,\n","    )\n","\n","    val_loader = DataLoader(\n","        valid_data,\n","        batch_size = batch_size,\n","        num_workers = num_workers,\n","        pin_memory = pin_memory,\n","        sampler = valid_sampler\n","    )\n","\n","    return train_loader, val_loader"]},{"cell_type":"markdown","metadata":{"id":"Dq6BbhPy7cYa"},"source":["### Transform Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1676027332951,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"1g-VJTVZ7cYa"},"outputs":[],"source":["def build_transforms(image_heigt, image_width): \n","    \n","    train_transform = A.Compose([\n","        A.Resize(\n","            height=image_heigt, \n","            width=image_width),\n","        A.Flip(p=0.5),\n","        A.ToGray(p=0.25),\n","        A.ColorJitter(p=0.25),\n","        A.Normalize(mean=(0.485, 0.456, 0.406), \n","                    std=(0.229, 0.224, 0.225)),\n","        ToTensorV2(),\n","        ],)\n","\n","    val_transform = A.Compose([\n","        A.Resize(height=image_heigt, width=image_width),\n","        A.Normalize(mean=(0.485, 0.456, 0.406), \n","                    std=(0.229, 0.224, 0.225)),\n","        ToTensorV2(),\n","        ],)\n","    \n","    return train_transform, val_transform"]},{"cell_type":"markdown","metadata":{"id":"tEYGjKJq7cYa"},"source":["### Evaluation Function "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1676027332952,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"v7kR8e987cYb"},"outputs":[],"source":["def check_accuracy(loader, model, pretrained=False, device=\"cuda\"):\n","  \n","  num_correct = 0\n","  num_pixels = 0\n","\n","  model.eval()\n","  with torch.no_grad():\n","    for x, y in loader:\n","      x = x.to(device)\n","      \n","      # compute probabilities\n","      if pretrained:\n","        probs = torch.nn.Softmax(model(x)['out'])\n","      else:\n","        probs = torch.nn.Softmax(model(x))\n","      \n","      # get predictions by choosing highest probability \n","      preds = torch.argmax(probs.dim, axis=1).cpu()\n","      num_correct += (preds == y).sum().item()\n","      num_pixels += torch.numel(preds)\n","\n","  print(\n","      f\"Got {num_correct}/{num_pixels} pixels correct with acc {num_correct/num_pixels*100:.2f}\"\n","      )"]},{"cell_type":"markdown","metadata":{"id":"kAbT4cqI7cYb"},"source":["### Save & Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1676027332952,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"DaR9JGOx7cYb"},"outputs":[],"source":["def save_checkpoint(state, filename=\"weights/potsdam_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(checkpoint, model):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])"]},{"cell_type":"markdown","metadata":{"id":"irpTsfQg7cYb"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"qllSL_QBA2UK"},"source":["### Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":206,"status":"ok","timestamp":1676036836152,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"G__vSjkx2QN1"},"outputs":[],"source":["# Specify number of classes for segmentation task\n","OUTPUT_CHANNELS = 4\n","\n","# Hyperparameters etc.\n","LEARNING_RATE = 1e-4\n","BATCH_SIZE = 8\n","NUM_EPOCHS = 4\n","\n","DICT_IMGS = imgs_dict\n","LABELS_DICT = labels_dict\n","KEYS = keys\n","IMAGE_HEIGHT = 1000  # 6000 originally\n","IMAGE_WIDTH = 1000  # 6000 originally\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","NUM_WORKERS = 2\n","PIN_MEMORY = True\n","\n","PRETRAINED = False\n","GEO_PRETRAINED = False\n","FREEZE = False"]},{"cell_type":"markdown","metadata":{},"source":["### Train Function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_fn(loader, model, optimizer, loss_fn, scaler):\n","    \n","  loss_sum = 0.0\n","  loop = tqdm(loader, desc=\"Training\")\n","\n","  model.train()\n","  for batch_idx, (data, targets) in enumerate(loop):\n","      \n","    data = data.to(device=DEVICE)\n","    targets = targets.type(torch.LongTensor).to(device=DEVICE)\n","\n","    # forward\n","    with torch.cuda.amp.autocast():\n","      if PRETRAINED or GEO_PRETRAINED:\n","        predictions = model(data)['out']\n","      else:\n","        predictions = model(data)\n","\n","      loss = loss_fn(predictions, targets)\n","\n","    # backward\n","    optimizer.zero_grad()\n","    scaler.scale(loss).backward()\n","    scaler.step(optimizer)\n","    scaler.update()\n","    \n","    # Comupte loss sum and count batches\n","    loss_sum += loss.item()\n","    batch_idx += 1\n","    \n","    # Update loop every 5 batches\n","    if not batch_idx % 5:\n","      loop.set_postfix(loss=loss_sum/(batch_idx))"]},{"cell_type":"markdown","metadata":{},"source":["### Train Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":248,"status":"ok","timestamp":1676036050370,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"8m6py8Jm7cYb"},"outputs":[],"source":["def main():\n","  \n","  # Specify tranfsforms for train and validation data\n","  train_transform, val_transform = build_transforms(\n","      image_heigt=IMAGE_HEIGHT, \n","      image_width=IMAGE_WIDTH)\n","\n","  # Initialize the data loader\n","  train_loader, val_loader = get_loaders(\n","      DICT_IMGS, \n","      LABELS_DICT,\n","      KEYS, \n","      BATCH_SIZE,\n","      train_transform,\n","      val_transform,\n","      NUM_WORKERS,\n","      PIN_MEMORY,)\n","\n","  # Load pre-trained DeepLapV3 ResNet101\n","  if PRETRAINED:\n","    MODEL = models.segmentation.deeplabv3_resnet101(weights=DeepLabV3_ResNet101_Weights.DEFAULT)\n","    MODEL.classifier = DeepLabHead(2048, OUTPUT_CHANNELS)\n","    MODEL.aux_classifier = None\n","\n","  # Load pre-trained ResNet50\n","  elif GEO_PRETRAINED:\n","    weights_backbone = resnet.ResNet50_Weights.SENTINEL2_RGB_SECO\n","    backbone = resnet50(weights=weights_backbone)\n","    MODEL = _fcn_resnet(backbone, OUTPUT_CHANNELS, aux=False)\n","\n","  # Load plain UNet\n","  else: \n","    MODEL = UNET(in_channels=3, out_channels=OUTPUT_CHANNELS)\n","\n","  # Train only classifier (last layer)\n","  if PRETRAINED or GEO_PRETRAINED:\n","    if FREEZE:\n","      for name, param in MODEL.named_parameters():\n","        if \"classifier\" not in name:\n","          param.requires_grad = False\n","  \n","  MODEL = MODEL.to(device=DEVICE)\n","\n","  # Class weights for multi-class classification\n","  # 0 - imprevious, \n","  # 1 - building, \n","  # 2 - low_vegetation,\n","  # 3 - tree\n","  weights = [0.8, 0.1, 1.0, 1.0]\n","  class_weights = torch.FloatTensor(weights).cuda()\n","\n","  CRITERION = nn.CrossEntropyLoss(weight = class_weights).to(device=DEVICE)\n","  OPTIMIZER = optim.Adam(MODEL.parameters(), lr=LEARNING_RATE)\n","  SCALER = torch.cuda.amp.GradScaler()\n","\n","  for epoch in range(NUM_EPOCHS):\n","      print(f\"Start epoch: {epoch+1}\")\n","      \n","      # Train model\n","      train_fn(train_loader, MODEL, OPTIMIZER, CRITERION, SCALER)\n","      \n","      # Save model\n","      checkpoint = {\n","          \"state_dict\": MODEL.state_dict(),\n","          \"optimizer\": OPTIMIZER.state_dict(),\n","      }\n","      save_checkpoint(checkpoint)\n","      \n","      # Check accuracy\n","      pre = PRETRAINED or GEO_PRETRAINED\n","      check_accuracy(val_loader, MODEL, pretrained=pre, device=DEVICE)\n","      \n","      print(f\"End epoch: {epoch+1} \\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":634111,"status":"ok","timestamp":1674749927795,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"ku3E0XlN7cYb","outputId":"172f8858-e521-485a-916c-8d3d6e176c31"},"outputs":[],"source":["# Train model\n","print(f\"Cuda available: {torch.cuda.is_available()}\")\n","main()"]},{"cell_type":"markdown","metadata":{"id":"9jamOwzR7cYb"},"source":["## Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tt-m-Jln7cYb"},"outputs":[],"source":["def model_predict(\n","    image, \n","    model, \n","    dataset, \n","    image_height, \n","    image_width, \n","    pretrained=False, \n","    device=\"cuda\", \n","    no_val_resize=False):\n","\n","    img = np.array(image.convert(\"RGB\"))\n","\n","    # Convert image to tensor & transform\n","    _, val_transform = build_transforms(\n","        image_heigt=image_height, \n","        image_width=image_width)\n","    \n","    if no_val_resize:\n","        val_transform.transforms.remove(val_transform.transforms[0])\n","    \n","    img = val_transform(image=img)\n","    pic = img['image']\n","\n","    model.eval()\n","    with torch.no_grad():\n","        # Use image + pretend batch size of 1\n","        pic = pic.to(device).unsqueeze(0)\n","\n","        # Compute probabilities\n","        if pretrained:\n","          probs = torch.nn.Softmax(model(pic)['out']).dim\n","        else:\n","          probs = torch.nn.Softmax(model(pic)).dim\n","        # Get predictions by choosing highest probability \n","        preds = torch.argmax(probs, axis=1).cpu().squeeze(0)\n","\n","        # Initialise empty arrays for RGB classes and RGB values\n","        classes = np.empty((preds.shape[0], preds.shape[1]), dtype=\"<S14\")\n","        img_rgb = np.empty((preds.shape[0], preds.shape[1], 3))\n","\n","        # Convert numeric labels to RGB classes\n","        for cl in dataset.bin_classes:\n","            classes[preds.detach().numpy() == dataset.bin_classes.index(cl)] = cl\n","            \n","        # Convert RGB classes to RGB values \n","        for key, rgb in dataset.RGB_classes.items():\n","            img_rgb[classes == str.encode(key), :] = rgb\n","\n","        return img_rgb.astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1939,"status":"ok","timestamp":1675087591289,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"IPQXRxLq7cYb","outputId":"76307ee1-179e-41b3-f77c-e1c8d0c42a7c"},"outputs":[],"source":["if PRETRAINED:\n","  trained_model = models.segmentation.deeplabv3_resnet101()\n","\n","# More than last classifier layer\n","\n","  trained_model.classifier = DeepLabHead(2048, OUTPUT_CHANNELS)\n","  trained_model.aux_classifier = None\n","  trained_model = trained_model.to(device='cuda')\n","\n","  load_checkpoint(torch.load(\"weights/love_checkpoint_DLV3biased_75acc.pth.tar\"), trained_model)\n","  \n","elif GEO_PRETRAINED:\n","  \n","  backbone = resnet50()\n","  trained_model = _fcn_resnet(backbone, OUTPUT_CHANNELS, aux=False)\n","  trained_model = trained_model.to(device='cuda')\n","  load_checkpoint(torch.load(\"love_checkpoint_RN50unbiased.pth.tar\"), trained_model)\n","  \n","else: \n","  trained_model = UNET(in_channels=3, out_channels=OUTPUT_CHANNELS).to(device='cuda')\n","  load_checkpoint(torch.load(\"weights/potsdam_checkpoint.pth.tar\"), trained_model)"]},{"cell_type":"markdown","metadata":{"id":"_hiLd9Vf7cYc"},"source":["### Ear Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209},"executionInfo":{"elapsed":6191,"status":"ok","timestamp":1674740079552,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"XF8i2D5w7cYc","outputId":"280fe4e4-dfde-4553-c695-3bb252f2fab3"},"outputs":[],"source":["input_img = Image.open(path+\"data/imagery/raw/BB_ML_0055_2018-05-04.jpg\")\n","\n","plt.imshow(input_img)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1090,"status":"ok","timestamp":1674740161972,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"NERHs3A6Sr_h","outputId":"5014ee4c-1adf-471f-c8f1-343fd30492c7"},"outputs":[],"source":["img_arr = np.array(input_img.convert(\"RGB\"))\n","img_arr.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":1062,"status":"ok","timestamp":1674749142606,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"e_A7JqUs7cYc","outputId":"546b4dae-a21a-411d-ca88-2ff355870d14"},"outputs":[],"source":["# Setting the cordinates for cropped image\n","x_left = 5500\n","x_right = x_left + 1000\n","\n","y_top = 2500\n","y_bottom = y_top + 1000\n","\n","# Cropped image of above dimension\n","# (It will not change original image)\n","input_img_cropped = input_img.crop((x_left, y_top, x_right, y_bottom))\n","\n","plt.imshow(input_img_cropped)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1674749930241,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"CMskU-nF7cYc","outputId":"5c7a3314-7088-42d6-febf-5f3941a0c85f"},"outputs":[],"source":["size = 800\n","predictions = model_predict(\n","    image=input_img_cropped, \n","    model=trained_model, \n","    dataset=dataset, \n","    image_height=size, \n","    image_width=size,\n","    pretrained=False, \n","    device=DEVICE,)\n","\n","predictions.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":378},"executionInfo":{"elapsed":1985,"status":"ok","timestamp":1674749932225,"user":{"displayName":"Yvonne B","userId":"16424637790358895407"},"user_tz":-60},"id":"rRMmHLbc3uvM","outputId":"ac7993e5-db58-4105-f44a-dcf26d24777c"},"outputs":[],"source":["# Acc 74 - weights = [0.9, 0.1, 0.9, 1.0]\n","im1 = Image.fromarray(predictions.astype(np.uint8)).resize((1000, 1000))\n","im2 = input_img_cropped\n","\n","f, ax = plt.subplots(1, 2, figsize=(10,5), sharey=True)\n","ax[0].imshow(im1)\n","ax[1].imshow(im2)\n","\n","f.tight_layout()\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["gV_cO9ee7cYW","GJ90XNO87cYY","UAB_U9AF7cYa","VLy5RBoD7cYa","Dq6BbhPy7cYa","tEYGjKJq7cYa","kAbT4cqI7cYb","HV5q3nu9A-HU","qllSL_QBA2UK","9jamOwzR7cYb"],"provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"penv","language":"python","name":"penv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"vscode":{"interpreter":{"hash":"eb7f5e66918d02d82412820c0a4aa3505fa06a6cc37c5a10c65e43bd94ac1a13"}}},"nbformat":4,"nbformat_minor":0}
