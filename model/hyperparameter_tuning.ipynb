{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, trange \n",
    "import glob\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double convolutional layer which is executed in every step of the u-net \n",
    "# conv layer takes as input number of input channels -> in_channels and outputs vice versa\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    # forward pass in the conv layer \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# design complete u-net shape \n",
    "# model takes as default 3 input channels and 6 output channels\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=3, out_channels=6, features=[64, 128, 256, 512],  # features -> num of input nodes at every stage in the model \n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):  # reverse the features i.o. to move upwards in the model \n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "        \n",
    "        # lowest stage in u-net \n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        # final conv layer: takes in 64 channels and outputs 1 channel by default \n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    # forward pass of the u-net model between stages \n",
    "    def forward(self, x):\n",
    "        skip_connections = []  # red arrows in the model representation \n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)  # one DoubleConv run-through \n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection with google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set path variable\n",
    "path = \"/content/drive/MyDrive/DS-Project/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists with paths to all image and label data \n",
    "imgs = glob.glob(path+'data/model_training/2_Ortho_RGB/sliced/*tif')\n",
    "labels = glob.glob(path+\"data/model_training/Labels_all/sliced/*tif\")\n",
    "\n",
    "# Create dictionary -> {key: 'link/to/image_or_label'}\n",
    "labels_dict = {label.split(\"/\")[-1].split(\".\")[0].rsplit('_', 1)[0] : label for label in labels}\n",
    "imgs_dict = {img.split(\"/\")[-1].split(\".\")[0].rsplit('_', 1)[0] : img for img in imgs}\n",
    "\n",
    "# Create list with all keys \n",
    "keys = sorted(list(set(imgs_dict)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotsdamDataset(Dataset):\n",
    "    def __init__(self, imgs_dict, labels_dict, keys, transform=None):\n",
    "        self.img_dir = imgs_dict\n",
    "        self.mask_dir = labels_dict\n",
    "        self.keys = keys\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.RGB_classes = {\n",
    "            'imprevious' : [255, 255, 225],\n",
    "            'building' : [0,  0, 255],\n",
    "            'low_vegetation' : [0, 255, 255],\n",
    "            'tree' : [0,  255,  0], \n",
    "            'car' : [ 255, 255, 0],\n",
    "            'background' : [255, 0, 0]\n",
    "            }  # in RGB\n",
    "        \n",
    "        self.bin_classes = ['imprevious', 'building', 'low_vegetation', 'tree', 'car', 'background']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_dir[self.keys[idx]]\n",
    "        mask_path = self.mask_dir[self.keys[idx]]\n",
    "        \n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"RGB\"))\n",
    "        \n",
    "        cls_mask = np.zeros(mask.shape) # dim: (6000, 6000, 3)\n",
    "        \n",
    "        cls_mask[(mask == self.RGB_classes['imprevious']).all(-1)] = self.bin_classes.index('imprevious')\n",
    "        cls_mask[(mask == self.RGB_classes['building']).all(-1)] = self.bin_classes.index('building')\n",
    "        cls_mask[(mask == self.RGB_classes['low_vegetation']).all(-1)] = self.bin_classes.index('low_vegetation')\n",
    "        cls_mask[(mask == self.RGB_classes['tree']).all(-1)] = self.bin_classes.index('tree')\n",
    "        cls_mask[(mask == self.RGB_classes['car']).all(-1)] = self.bin_classes.index('car')\n",
    "        cls_mask[(mask == self.RGB_classes['background']).all(-1)] = self.bin_classes.index('background')\n",
    "        cls_mask = cls_mask[:,:,0] # omit last dimension (, , 3) -> RGB  \n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=cls_mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-/Validation-Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PotsdamDataset(imgs_dict, labels_dict, keys)\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(\n",
    "    imgs_dict,\n",
    "    labels_dict,\n",
    "    keys,\n",
    "    batch_size,\n",
    "    train_transform,\n",
    "    val_transform,\n",
    "    num_workers = 2,\n",
    "    pin_memory = True,\n",
    "):\n",
    "    \n",
    "    train_data = PotsdamDataset(\n",
    "        imgs_dict = imgs_dict,\n",
    "        labels_dict = labels_dict,\n",
    "        keys = keys, \n",
    "        transform = train_transform,\n",
    "    )\n",
    "    \n",
    "    valid_data = PotsdamDataset(\n",
    "        imgs_dict = imgs_dict,\n",
    "        labels_dict = labels_dict,\n",
    "        keys = keys, \n",
    "        transform = val_transform,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers,\n",
    "        pin_memory = pin_memory,\n",
    "        sampler = train_sampler,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        valid_data,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers,\n",
    "        pin_memory = pin_memory,\n",
    "        sampler = valid_sampler\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transforms(image_heigt, image_width): \n",
    "    \n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=image_heigt, width=image_width),\n",
    "        A.Flip(p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                    std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "        ],)\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=image_heigt, width=image_width),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                    std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "        ],)\n",
    "    \n",
    "    return train_transform, val_transform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, loader, criterion, device):\n",
    "\n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "\n",
    "  model.eval()\n",
    "  loop = tqdm(loader, desc=\"Evaluating\", leave=False)\n",
    "\n",
    "  with torch.no_grad():\n",
    "\n",
    "    for (x, y) in loop:\n",
    "\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      y_pred = model(x)\n",
    "\n",
    "      loss = criterion(y_pred, y)\n",
    "\n",
    "      acc = compute_accuracy(y_pred, y)\n",
    "\n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += acc.item()\n",
    "\n",
    "  return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "  elapsed_time = end_time - start_time\n",
    "  elapsed_mins = int(elapsed_time / 60)\n",
    "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "  return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup wandb \n",
    "!pip install wandb --upgrade\n",
    "import pprint\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'valiadtion_accuracy',\n",
    "    'goal': 'maximize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict = {\n",
    "    'batch_size': {\n",
    "        # integers between 2 and 32\n",
    "        # with evenly-distributed logarithms \n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q': 8,\n",
    "        'min': 2,\n",
    "        'max': 32,\n",
    "        },\n",
    "    'learning_rate': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0,\n",
    "        'max': 0.1\n",
    "        },\n",
    "    'epochs': {'values': [2, 4, 8]\n",
    "        },\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"ds_project\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HP Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader, model, optimizer, criterion, scaler, device):\n",
    "    \n",
    "    model.train()\n",
    "    loop = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        \n",
    "        data = data.to(device)\n",
    "        targets = targets.type(torch.LongTensor).to(device)\n",
    "\n",
    "        # forward\n",
    "        # with torch.cuda.amp.autocast():\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "          predictions = model(data)\n",
    "          loss = criterion(predictions, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update tqdm loop\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 1000  # 2000 originally\n",
    "IMAGE_WIDTH = 1000  # 2000 originally\n",
    "\n",
    "IMGS_DICT = imgs_dict\n",
    "LABELS_DICT = labels_dict\n",
    "KEYS = keys\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# main function with wandb\n",
    "def main(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        \n",
    "        train_transforms, val_transforms = build_transforms(\n",
    "            image_heigt=IMAGE_HEIGHT,\n",
    "            image_width=IMAGE_WIDTH\n",
    "        )\n",
    "        \n",
    "        train_loader, validation_loader = get_loaders(\n",
    "            imgs_dict=IMGS_DICT,\n",
    "            labels_dict=LABELS_DICT,\n",
    "            keys=KEYS,\n",
    "            batch_size=config.batch_size,\n",
    "            train_transform=train_transforms,\n",
    "            val_transform=val_transforms,\n",
    "            num_workers = NUM_WORKERS,\n",
    "            pin_memory = PIN_MEMORY)\n",
    "        \n",
    "        # specify loss function \n",
    "        CRITERION = nn.CrossEntropyLoss()\n",
    "        CRITERION = CRITERION.to(DEVICE)\n",
    "        \n",
    "        # initialize model and optimizer\n",
    "        MODEL = UNET(in_channels=3, out_channels=6).to(device=DEVICE)\n",
    "        OPTIMIZER = optim.Adam(MODEL.parameters(), lr=config.learning_rate)\n",
    "        \n",
    "        SCALER = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        \n",
    "    for epoch in trange(config.epochs, desc=\"Epochs\"):\n",
    "\n",
    "        start_time = time.monotonic()\n",
    "        \n",
    "        train_fn(loader=train_loader, \n",
    "                 model=MODEL, \n",
    "                 optimizer=OPTIMIZER, \n",
    "                 criterion=CRITERION, \n",
    "                 scaler=SCALER,\n",
    "                 device=DEVICE)\n",
    "        \n",
    "        training_loss, training_accuracy = evaluate_fn(model=MODEL, \n",
    "                                                       loader=train_loader, \n",
    "                                                       criterion=CRITERION, \n",
    "                                                       device=DEVICE)\n",
    "        \n",
    "        validation_loss, validation_accuracy = evaluate_fn(model=MODEL, \n",
    "                                                           loader=validation_loader, \n",
    "                                                           criterion=CRITERION, \n",
    "                                                           device=DEVICE)\n",
    "                \n",
    "        end_time = time.monotonic()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {training_loss:.3f} | Train Acc: {training_accuracy*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {validation_loss:.3f} |  Val. Acc: {validation_accuracy*100:.2f}%')\n",
    "        \n",
    "        wandb.log({\"epoch\": epoch, \n",
    "                   \"validation_accuracy\": validation_accuracy,\n",
    "                   \"validation_loss\": validation_loss}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, main, count=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb7f5e66918d02d82412820c0a4aa3505fa06a6cc37c5a10c65e43bd94ac1a13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
